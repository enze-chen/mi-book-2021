{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5334554e",
   "metadata": {},
   "source": [
    "# Advanced ML methods notebook\n",
    "\n",
    "*Authors: Enze Chen and Mark Asta (University of California, Berkeley)*\n",
    "\n",
    "```{note}\n",
    "This is an interactive exercise, so you will want to click the {fa}`rocket` and open the notebook in DataHub (or Colab for non-UCB students).\n",
    "```\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "This notebook contains a series of exercises that explore some advanced ML models that we didn't have enough time to cover during the live session.\n",
    "While it isn't required that you use these methods in your research, they could give some interesting results.\n",
    "By the end of this notebook, you should be able to:\n",
    "1. Build decision trees and articulate their strengths and weaknesses.\n",
    "1. Build random forests and artibulate their strengths and weaknesses.\n",
    "1. Evaluate when unsupervised learning is useful and how to perform $k$-means clustering.\n",
    "1. Define dimensionality reduction and articulate why/when it's useful.\n",
    "\n",
    "We will rely on the [scikit-learn](https://scikit-learn.org/stable/) package for implementing these algorithms.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "We have tried our best to present the following material in a visual, intuitive, and applied manner, but as these are more advanced methods, there may be several concepts that are new to you. \n",
    "Please don't be concerned if something doesn't make sense right away, and know that we're always happy to answer your questions!\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32d824",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "These exercises are grouped into the following sections:\n",
    "\n",
    "1. [Decision trees](#Decision-trees)\n",
    "1. [Random forests](#Random-forests)\n",
    "1. [Unsupervised learning](#Unsupervised-learning)\n",
    "1. [$k$-means clustering](#$k$-means-clustering)\n",
    "1. [Dimensionality reduction](#Dimensionality-reduction)\n",
    "\n",
    "### Import the common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(8,6),       # Increase figure size\n",
    "                     'font.size':20,               # Increase font size\n",
    "                     'mathtext.fontset':'cm',      # Change math font to Computer Modern\n",
    "                     'mathtext.rm':'serif',        # Documentation recommended follow-up\n",
    "                     'lines.linewidth':5,          # Thicker plot lines\n",
    "                     'lines.markersize':12,        # Larger plot points\n",
    "                     'axes.linewidth':2,           # Thicker axes lines (but not too thick)\n",
    "                     'xtick.major.size':8,         # Make the x-ticks longer (our plot is larger!)\n",
    "                     'xtick.major.width':2,        # Make the x-ticks wider\n",
    "                     'ytick.major.size':8,         # Ditto for y-ticks\n",
    "                     'ytick.major.width':2,        # Ditto for y-ticks\n",
    "                     'xtick.direction':'in', \n",
    "                     'ytick.direction':'in'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e566de",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "A decision tree is a **nonlinear** model that uses sequence of conditional statements about the input features to make a final prediction.\n",
    "The resulting model resembles a flowchart and is similar to how humans might make decisions, such as:\n",
    "\n",
    "![decision tree class](../../assets/fig/week_1/04/decision_tree_class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e758b31",
   "metadata": {},
   "source": [
    "_Note that the above is only an illustration and we do not advocate skipping class_. üòú\n",
    "\n",
    "In any case, you'll note that the decision tree starts at the **root node** and there is a binary **split** at each decision node (blue ellipses) in the tree. \n",
    "The **features** used to make the decisions _can repeat_ (they do not in this example), and it can be a categorical feature (some class or binary, like the `is it recorded?` node) _or_ a numerical feature (some threshold, like the `when did I sleep?` node).\n",
    "The final decision is made at the **leaf nodes**, which are colored orange and green in the above example.\n",
    "\n",
    "**Pause and reflect**: Is the above decision tree used to solve a classification or a regression problem?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7e47d",
   "metadata": {},
   "source": [
    "### Benefits of decision trees\n",
    "\n",
    "There are several nice aspects to using decision trees, including:\n",
    "- Works for **classification and regression problems** (discussed more below).\n",
    "- Works with **categorical and numerical features** (discussed above).\n",
    "- **Easy to interpret** the model and how predictions are being made.\n",
    "- **Highly flexible** in how splits (decisions) are made.\n",
    "- Features **don't need to be standardized**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d52d2f",
   "metadata": {},
   "source": [
    "### Training a decision tree\n",
    "\n",
    "Training a decision tree is a little complicated so we won't delve into the details, but as you might imagine, the cost function for regression trees can be our familiar mean squared error.\n",
    "However, this cost function is now _local_ and re-evaluated at each decision node, so the decision tree construction is a **greedy** procedure.\n",
    "For classification tasks, the cost function changes to a different metric, common ones being the [Gini impurity](https://victorzhou.com/blog/gini-impurity/) or the [information gain (entropy)](https://victorzhou.com/blog/information-gain/).\n",
    "Notably, based on the cost function, the algorithm **learns the best feature** to split on at each node for the \"optimal split\" based on the training data; the user does not specify exactly which feature to consider.\n",
    "The user does have to supply an input design matrix and an output target vector, which makes a decision tree a **supervised learning** algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8602e8",
   "metadata": {},
   "source": [
    "### Decision trees in scikit-learn\n",
    "\n",
    "OK, now that we have a little bit of the theory under our belts, let's jump into building decision trees.\n",
    "The [`sklearn.tree`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) module contains the [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) classes.\n",
    "\n",
    "You may notice that these constructors have quite a lot of input arguments, which gives us our first experience working with **hyperparameters**. \n",
    "Recall that our model _parameters_ ($\\vec{\\theta}$ in linear regression, or the splitting criteria from above) are _learned from the data_ through an optimization procedure.\n",
    "Hyperparameters, on the other hand, are _chosen by the user_ and typically _optimized using a cross-validation procedure_ where we search through a list of candidate values for each hyperparameter and pick the best one based on the validation error.\n",
    "This is called **hyperparameter optimization** and is really important to building a good ML model.\n",
    "\n",
    "One such hyperparameter we might choose is the `max_depth` which determines how many layers our tree will have (the scikit-learn default is as many as necessary).\n",
    "In the example below, we will return to the atomic weight regression problem with the atomic number as the single, numerical feature, and you can see how changing `max_depth` changes the model performance.\n",
    "\n",
    "The following code looks a little complicated because we tried to use [widgets](https://ipywidgets.readthedocs.io/en/latest/) to make it interactive, but note how the first two code blocks in the `train_and_plot()` function are **very similar** to the `LinearRegression` model in scikit-learn.\n",
    "This design is intentional and makes it very straightforward to experiment with different ML models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor       # ML model\n",
    "from ipywidgets import interact, IntSlider, Layout   # cool package for interactivity\n",
    "\n",
    "df = pd.read_csv('../../assets/data/week_1/04/elem_props.csv')\n",
    "X = df[['atomic_number']]\n",
    "y = df['atomic_mass']\n",
    "\n",
    "def train_and_plot(depth):\n",
    "    dt = DecisionTreeRegressor(max_depth=depth)      # decision tree constructor\n",
    "    dt.fit(X, y)                                     # fit the tree\n",
    "    y_hat = dt.predict(X)                            # make predictions\n",
    "    train_err = mean_squared_error(y_hat, y, squared=False)\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    y_pred = cross_val_predict(dt, X, y, cv=kfold)\n",
    "    val_err = mean_squared_error(y_pred, y, squared=False)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X, y, label='data')\n",
    "    ax.plot(X, y_hat, 'k', label='predictions')\n",
    "    ax.text(1, 235, f'training RMSE: {train_err:.3f}')\n",
    "    ax.text(1, 210, f'validation RMSE: {val_err:.3f}')\n",
    "    ax.set_xlabel('atomic number')\n",
    "    ax.set_ylabel('atomic mass')\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "depth_widget = IntSlider(value=1, min=1, max=8, step=1,\n",
    "                         description='Maximum tree depth',\n",
    "                         style={'description_width':'150px'}, continuous_update=True,\n",
    "                         layout=Layout(width='400px', height='30px'))\n",
    "\n",
    "interact(train_and_plot, depth=depth_widget);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6506a2",
   "metadata": {},
   "source": [
    "**Pause and reflect**: Why is the prediction line so jagged?\n",
    "\n",
    "**Answer**: After the maximum depth has been reached, it will only predict the mean of the remaining training points in that leaf node.\n",
    "\n",
    "----\n",
    "\n",
    "Hopefully you saw that by increasing the `max_depth` of the decision tree, we can get better and better fits to our training data, eventually decreasing the _training error_ all the way down to 0!\n",
    "\n",
    "**Pause and reflect**: What drawback of decision trees does this behavior suggest?\n",
    "\n",
    "<!-- solution-begin -->\n",
    "**Answer**: High variance! Decision trees are _extremely sensitive_ to the input data.\n",
    "<!-- solution-begin -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c444890",
   "metadata": {},
   "source": [
    "### Visualizing a decision tree\n",
    "\n",
    "Similar to our very first example where we visualized our \"decision tree\" for attending class, we can programmatically create visualizations of our decision tree models using the [`plot_tree(decision_tree)`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) function in the `sklearn.tree` module.\n",
    "This is relatively new function that is quite customizable, giving us the option to specify the feature labels and class labels.\n",
    "We demonstrate it below for the classification problem of a metallic element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "df = pd.read_csv('../../assets/data/week_1/04/elem_props.csv')\n",
    "features = ['row', 'group']\n",
    "X = df[features]\n",
    "y = df['is_metal']\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "dt.fit(X, y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_tree(dt, feature_names=features, class_names=['non-metal', 'metal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd25787",
   "metadata": {},
   "source": [
    "Now you can see exactly how the tree made each decision for classifying whether a material was a metal or non-metal.\n",
    "For example, it makes sense that the first split is vertical between groups 13 and 14, where if the group number is 13 or less, we proceed down a \"metal\" branch.\n",
    "The leaf nodes are those without arrows underneath, and the class label that's shown is the label that will be predicted if a material lands on that leaf as a result of traversing the tree.\n",
    "Notice that features can be repeated, even in successive layers of the tree.\n",
    "**Hooray for interpretability**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45ee38",
   "metadata": {},
   "source": [
    "## Random forests\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "We hope that you've enjoyed the presentation on decision trees.\n",
    "It's a cool, intuitive, and interpretable ML algorithm that's good to know.\n",
    "And it makes for some pretty visuals. üôÇ\n",
    "\n",
    "But there is one _huge problem_ with decision trees and that is their **high variance**.\n",
    "As you saw, decision trees are capable of partitioning the training data in a very intricate way, which means that the decision tree is very sensitive to noise/error in the training data, which can often lead to **overfitting** or simply inaccuracies.\n",
    "Once again, it seems like we're at the mercy of the **bias-variance tradeoff** (ha!), where we're either going to end up with large errors due to high-bias/low-complexity models (like linear regression and logistic regression) or high-variance/high-complexity models (like decision trees, or neural networks).\n",
    "There are ways of improving the performance of decision trees using hyperparameter optimization or _pruning_ (eliminating branches of the tree), but _it would be great if there was an algorithm that could land us at the sweet spot without too much hand tuning_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c5a75",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Luckily for us, there _is_ such an algorithm, which we will try to motivate in this section.\n",
    "\n",
    "**Idea 1**: Because we're proud of democracy, we might consider an algorithm that trains not one, not two, but _many_ decision trees and then have them \"vote\" on the final prediction.\n",
    "By \"vote,\" we mean we take the _mode_ of the predictions from every decision tree if it's a _classification_ task, and the _mean_ of the predictions from every decision tree if it's a _regression_ problem.\n",
    "In other words, we are **aggregating** the outputs and performance of an **ensemble** of decision trees‚Äîtwo heads are better than one.\n",
    "\n",
    "**Idea 2**: But wait!\n",
    "If we use the same training data for training each decision tree in this ensemble model, we're actually going to end up with pretty similar (**correlated**) trees... which means we'll perform the same as using just one decision tree.\n",
    "How can we inject some variation... maybe with some randomness?\n",
    "Let's **bootstrap** the training data, which means from the $m$ data points in our whole training dataset, we will **randomly sample with replacement** $m$ examples to form the training dataset for decision tree #1, randomly sample a different set of $m$ examples for training decision tree #2, etc.\n",
    "\n",
    "**Idea 3**: The fun doesn't have to stop there.\n",
    "We know that when we have a lot of features (degrees of freedom), we increase the model complexity and run the risk of overfitting to the training data.\n",
    "So, maybe when we're deciding which feature to split on at each node, we can just consider **a subset of the features** instead of all the features.\n",
    "This subset can be a randomly-sampled portion (w/o replacement is fine) of the features at each decision node in every decision tree.\n",
    "This further decorrelates the decision trees and decreases the variance of the model.\n",
    "\n",
    "Combining ideas 1 and 2 is called **bagging** (**b**ootstrap **agg**regat**ing**), and when we combine them with idea 3 to create an ensemble model of decision trees, we get one of the üêê ML algorithms: **random forests**.\n",
    "An earlier version of this algorithm was invented by [Tin Kam Ho](https://researcher.watson.ibm.com/researcher/view.php?person=us-tho) in 1995, while more-or-less the exact version described here was [invented in 2001 by Leo Breiman](https://link.springer.com/article/10.1023/A:1010933404324), who was a UC Berkeley PhD student ('54) and Professor of Statistics for 25 years! üêª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b8adf",
   "metadata": {},
   "source": [
    "### Random forests in scikit-learn\n",
    "\n",
    "Scikit-learn has a large collection of [ensemble models](https://scikit-learn.org/stable/modules/ensemble.html), among which are the [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "You'll note that these classes have many of the same input parameters as the decision tree classes, with a few additional ones due to ensembling.\n",
    "We invite you to read the documentation carefully to understand what each input means.\n",
    "The default values for the constructor aren't bad.\n",
    "\n",
    "In the cell below, we use the `RandomForestRegressor` to model the bandgap prediction problem that was presented in the \"Featurization\" notebook.\n",
    "We intentionally restrict the set of features using the following code, which you can consider doing for your own project (everything else is the same as the previous example):\n",
    "```python\n",
    "features = ['Number', 'MendeleevNumber', 'AtomicWeight', 'MeltingT', \n",
    "            'Column', 'Row', 'CovalentRadius', 'Electronegativity', \n",
    "            'NsValence', 'NpValence', 'NdValence', 'NfValence', 'NValence', \n",
    "            'NsUnfilled', 'NpUnfilled', 'NdUnfilled', 'NfUnfilled', 'NUnfilled', \n",
    "            'GSvolume_pa', 'GSbandgap', 'GSmagmom', 'SpaceGroupNumber']\n",
    "stats = ['mean']   # we can also add 'minimum', 'maximum', 'range', 'avg_dev', 'mode'\n",
    "featurizer = ElementProperty(data_source='magpie',\n",
    "                             features=features,\n",
    "                             stats=stats)\n",
    "# featurizer = ElementProperty.from_preset('magpie')   # instead of this\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and construct the arrays\n",
    "df = pd.read_csv('../../assets/data/week_1/04/band_gap_featurized.csv')\n",
    "display(df.head())\n",
    "X = df.loc[:, 'MagpieData mean Number':]\n",
    "y = df['gap expt']\n",
    "\n",
    "# Train the model and perform CV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "y_hat = cross_val_predict(rf, X, y, cv=kfold)\n",
    "\n",
    "# Plot the parity plot\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.scatter(y, y_hat, s=30)\n",
    "ax.plot(y, y, 'k', zorder=-5)\n",
    "ax.set_xlabel('actual band gap (eV)')\n",
    "ax.set_ylabel('predicted band gap (eV)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482dc643",
   "metadata": {},
   "source": [
    "The predictions still aren't perfect (band gap is a hard problem!), but they're better than our linear regression model.\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "One of the cool aspects of random forests is that we can calculate the **importance score of each feature** that measures a degree of its \"influence\" on predicting the output property.\n",
    "The importance scores will be normalized to sum to `1.0`, and you can read more online about exactly how these scores are calculated, if you want.\n",
    "The code below will do this for the band gap prediction problem up above and rank the features in order of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea1e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)                                              # we have to explicitly fit the model first\n",
    "features = np.array([s[11:] for s in X.columns])          # remove the \"MagpieData \" prefix\n",
    "importances = rf.feature_importances_                     # get the importances from the attribute\n",
    "importances_sorted = sorted(importances, reverse=False)   # sort importances, ascending for plotting quirk\n",
    "indices_sorted = np.argsort(importances)                  # get the indices for indexing into features\n",
    "features_sorted = features[indices_sorted]                # sort the features too for labeling the plot\n",
    "\n",
    "# create the bar chart for importance scores in ranked order\n",
    "yvals = np.arange(len(indices_sorted))    # bar/tick positions\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(yvals, importances_sorted)\n",
    "ax.set_yticks(yvals)\n",
    "ax.set_yticklabels(features_sorted)\n",
    "ax.set_ylabel('feature')\n",
    "ax.set_xlabel('importance score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dede5e54",
   "metadata": {},
   "source": [
    "**Pause and reflect**: Can you rationalize the feature importances above?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300e57e",
   "metadata": {},
   "source": [
    "### Don't say it... don't say it...\n",
    "\n",
    "With many caveats, random forests are _one of the best \"out of the box\" ML algorithms_.\n",
    "They work very well on a wide variety of regression and classification problems and are interpretable (e.g., feature importance).\n",
    "Like linear regression, this is a good model to use as a \"benchmark\" before trying other, more complex models.\n",
    "With scikit-learn, building a random forest follows the same sequence of steps as linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33748859",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <center><b>~ BREAK ~</b></center>\n",
    "    At this point, we highly suggest you give yourself a decent break before continuing further. ü•∞\n",
    "    Get a drink of water, go grab a bite, or at least stand up and stretch to give your eyes a break.\n",
    "    You all are awesome!! üôå\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b497b2",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Up until this point, we've only looked at supervised learning algorithms, where the algorithm must be given the inputs $X$ and outputs $\\vec{y}$ to fit some parameters $\\vec{\\theta}$.\n",
    "Now we will launch into a short discussion of **unsupervised learning** algorithms, where the algorithm is only given the [same] input design matrix $X$, _but not the output target vector_ $\\vec{y}$.\n",
    "We'll discuss some concrete examples of such algorithms and why we might want to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f0ce55",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Goals of unsupervised learning\n",
    "\n",
    "After reading the above, one might think, \"Well, if an unsupervised learning algorithm is provided even less information than a supervised learning algorithm, wouldn't it necessarily be less powerful?\"\n",
    "This would be true if the algorithms were trying to achieve the same goal, but since they aren't, unsupervised learning algorithms can still be very useful.\n",
    "\n",
    "Recall in supervised learning, we're trying to figure out how to map a set of inputs in $X$ to an output $\\vec{y}$ as accurately as possible.\n",
    "In unsupervised learning, we're no longer concerned about any target property/label, but rather we're curious about the **inherent structure** in the data, such as whether there are clusters or trends along certain features.\n",
    "The goal always comes first, then the algorithms (which scientists decided to name this way)‚Äînot the other way around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d9811",
   "metadata": {},
   "source": [
    "## $k$-means clustering\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Since one of the goals of unsupervised learning is to reveal clusters in your data, let's discuss one of the simplest algorithms to achieve this goal: $k$**-means**.\n",
    "The intuition behind this algorithm is to find $k$ cluster centers, or **centroids**, such that all data points that are closest to the same centroid are assigned the same cluster.\n",
    "Note that when we say two data points belong to the same cluster, we're **not** saying they have the same class/label (like in classification problems), but rather that they are near each other _in feature space_.\n",
    "\n",
    "$k$-means is an **iterative algorithm** consisting of two steps:\n",
    "\n",
    "0. First, at the very beginning, the user initializes $k$ centroids by their choosing, either randomly or [intelligently](https://en.wikipedia.org/wiki/K-means++).\n",
    "1. **Assignment step**: For each _data point_, assign it to the closest centroid by Euclidean distance.\n",
    "2. **Update step**: For each _centroid_, recalculate its position to be the centroid of all the data points that were assigned to it in the previous step.\n",
    "3. Repeat steps **1** and **2** until the centroids converge.\n",
    "\n",
    "To illustrate how this algorithm works, we provide an implementation below where we perform $k$-means manually.\n",
    "The data are points in 2D space randomly scattered around three centers and we use a star ‚òÖ to represent the centroid. \n",
    "The points are colored based on the current centroid (cluster) they're assigned to, and you can see them change color (clusters) as the algorithm progresses and as the centroids update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment step\n",
    "def assign(points, centroids, clusters):\n",
    "    for j, p in enumerate(points):\n",
    "        dist = np.linalg.norm(centroids - p, axis=1)   # calculate distance between p and each centroid\n",
    "        clusters[j] = np.argmin(dist)                  # reassign the point to the nearest centroid (cluster)\n",
    "    return clusters\n",
    "\n",
    "# update step\n",
    "def update(points, centroids, clusters):\n",
    "    for i in range(k):\n",
    "        id_mask = clusters == i                           # find which points belong to the ith cluster\n",
    "        centroids[i] = np.mean(points[id_mask], axis=0)   # recalculate the centroid based on currently-assigned data points\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2978acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# load in data and initialize clusters array\n",
    "points = np.load('../../assets/data/week_1/04/kmeans_data.npy')\n",
    "clusters = np.zeros(len(points), dtype=int)\n",
    "\n",
    "# initialize 3 centroids randomly\n",
    "k = 3\n",
    "rng = np.random.default_rng(seed=1)\n",
    "centroids = rng.choice(np.arange(2 * k, dtype=float), size=(k, 2))\n",
    "clusters = assign(points, centroids, clusters)\n",
    "\n",
    "# plotting boilerplate\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "h = ax.scatter([], [])\n",
    "j = ax.scatter([], [])\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    return h, j,\n",
    "\n",
    "def animate(kk):\n",
    "    global points, centroids, clusters, h, j\n",
    "    clusters = assign(points, centroids, clusters)\n",
    "    h.remove()\n",
    "    h = ax.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c=[f'C{i}' for i in range(k)])\n",
    "    j.remove()\n",
    "    j = ax.scatter(points[:, 0], points[:, 1], c=[f'C{i}' for i in clusters], alpha=0.4)\n",
    "    centroids = update(points, centroids, clusters)\n",
    "    return h, j,\n",
    "\n",
    "plt.rcParams.update({'animation.html': 'jshtml', 'font.size':12})\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=5, interval=100, repeat=False)\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b0a2f",
   "metadata": {},
   "source": [
    "OK, so that was a toy example just to illustrate how the algorithm works (repeating the assign-update steps) so you can understand how the clusters are assigned in higher dimensions.\n",
    "While this is a fairly intuitive way to identify clusters in your data, there are [a few issues](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages) with $k$-means, including:\n",
    "- Has poor performance in higher dimensions (if there are a lot of features)\n",
    "- Sensitive on the initial choice of cluster centroids.\n",
    "- User must manually choose the value of $k$ (for example, the algorithm inherently does not know the above data is in three clusters).\n",
    "\n",
    "### Using KMeans from scikit-learn\n",
    "\n",
    "With the [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) class in scikit-learn, we can automate a lot of the previous code we wrote.\n",
    "You can read online ([such as this blog](https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb)) for ways to determine the optimal $k$ value.\n",
    "There are also [many more clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html) in scikit-learn besides $k$-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb63b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k)       # initialize the model and k value\n",
    "kmeans.fit(points)                  # do the iterative steps to finalize centroids\n",
    "clusters = kmeans.predict(points)   # this predicts the cluster labels, not an output property!\n",
    "centers = kmeans.cluster_centers_   # get the final centroids\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(points[:, 0], points[:, 1], c=[f'C{i}' for i in clusters], alpha=0.4)\n",
    "ax.scatter(centers[:, 0], centers[:, 1], s=200, marker='*', c=[f'C{i}' for i in range(k)])\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052396c5",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "In addition to clustering, unsupervised learning is commonly used for **dimensionality reduction** to simplify the representation of your data in a lower dimensional space.\n",
    "What do we mean by \"dimension?\"\n",
    "Let's say your data (design matrix) looks like:\n",
    "\n",
    "$$ X = \\begin{bmatrix} x_1^{(1)} & x_2^{(1)} & x_3^{(1)} \\\\ \n",
    "                       x_1^{(2)} & x_2^{(2)} & x_3^{(2)} \\\\ \n",
    "                       \\vdots & \\vdots & \\vdots \\\\\n",
    "                       x_1^{(m)} & x_2^{(m)} & x_3^{(m)} \\end{bmatrix} $$\n",
    "                       \n",
    "where each data point (row) has three features that could conceivably be coordinates in 3D space.\n",
    "Therefore, we might say that the data in $X$ \"lives in 3 dimensions.\"\n",
    "Analogously, when we have a design matrix with $n$ features, we say the data lives in $n$-dimensional space.\n",
    "Since $n$-dimensional space is complicated and hard to visualize for $n > 3$ (or $n > 2$, really), we would like to see if there is a way to **project** our data into 2D **while preserving the structure** of the data in the original $n$-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25fdd26",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)\n",
    "\n",
    "As you might expect, there are many ways to do this projection, ultimately boiling down to whether the projection is done linearly or nonlinearly.\n",
    "One type of **linear projection** is **principal components analysis (PCA)**, which projects your data onto the subspace spanned by the principal components (PCs), which are orthogonal vectors pointing in the directions of **greatest variance**.\n",
    "As an example, if you have a cloud of points in 3D shaped like a football, then the first principal component (direction of greatest variance) points from the center of the football towards one of the ends.\n",
    "The second principal component then points from the center out towards the bulging side.\n",
    "\n",
    "PCA is a very common dimensionality reduction technique to help visualize high-dimensional datasets in 2D.\n",
    "Once you calculate the lower-dimensional representation, you can perform other calculations such as [$k$-means clustering](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html), [regression](https://www.statology.org/principal-components-regression/), and more.\n",
    "While scikit-learn has a [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) package, we will leave it up to you to read more about it as we don't want to overload you with code and can't come up with a good motivating example.\n",
    "If you are interested in this for your research, let us know and we'd be happy to chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cceb409",
   "metadata": {},
   "source": [
    "### _t_-distributed stochastic neighbor embedding (t-SNE)\n",
    "\n",
    "One of the most popular forms of **nonlinear** dimensionality reduction is the _t_-distributed stochastic neighbor embedding algorithm, or **t-SNE**, which has the potential to generate very pretty and surprisingly visualizations.\n",
    "The motivation behind this algorithm is one that tries to preserve spatial similarity, such that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points.\n",
    "t-SNE plots are tricky to make because they're **very sensitive to hyperparameter optimization**, as described in [this Distill post](https://distill.pub/2016/misread-tsne/).\n",
    "Scikit-learn has an implementation in their [`TSNE`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) package, and we invite you to read the documentation if you want to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e529c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "This concludes the bonus content on advanced ML models.\n",
    "\n",
    "If you have any questions, please do not hesitate to ask on Slack!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
