{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5334554e",
   "metadata": {},
   "source": [
    "# Advanced ML methods notebook\n",
    "\n",
    "*Authors: Enze Chen and Mark Asta (University of California, Berkeley)*\n",
    "\n",
    "```{note}\n",
    "This is an interactive exercise, so you will want to click the {fa}`rocket` and open the notebook in DataHub (or Colab for non-UCB students).\n",
    "```\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "This notebook contains a series of exercises that explore some advanced ML models that we didn't have enough time to cover during the live session.\n",
    "While it isn't required that you use these methods in your research, they could give some interesting results.\n",
    "By the end of this notebook, you should be able to:\n",
    "1. Build decision trees and articulate their strengths and weaknesses.\n",
    "1. Build random forests and artibulate their strengths and weaknesses.\n",
    "1. Evaluate when unsupervised learning is useful and how to perform $k$-means clustering.\n",
    "1. Define dimensionality reduction and articulate why/when it's useful.\n",
    "\n",
    "We will rely on the [scikit-learn](https://scikit-learn.org/stable/) package for implementing these algorithms.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "We have tried our best to present the following material in a visual, intuitive, and applied manner, but as these are more advanced methods, there may be several concepts that are new to you. \n",
    "Please don't be concerned if something doesn't make sense right away, and know that we're always happy to answer your questions!\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32d824",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "These exercises are grouped into the following sections:\n",
    "\n",
    "1. [Decision trees](#Decision-trees)\n",
    "1. [Random forests](#Random-forests)\n",
    "1. [Unsupervised learning](#Unsupervised-learning)\n",
    "1. [$k$-means clustering](#$k$-means-clustering)\n",
    "1. [Dimensionality reduction](#Dimensionality-reduction)\n",
    "\n",
    "### Import the common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(8,6),       # Increase figure size\n",
    "                     'font.size':20,               # Increase font size\n",
    "                     'mathtext.fontset':'cm',      # Change math font to Computer Modern\n",
    "                     'mathtext.rm':'serif',        # Documentation recommended follow-up\n",
    "                     'lines.linewidth':5,          # Thicker plot lines\n",
    "                     'lines.markersize':12,        # Larger plot points\n",
    "                     'axes.linewidth':2,           # Thicker axes lines (but not too thick)\n",
    "                     'xtick.major.size':8,         # Make the x-ticks longer (our plot is larger!)\n",
    "                     'xtick.major.width':2,        # Make the x-ticks wider\n",
    "                     'ytick.major.size':8,         # Ditto for y-ticks\n",
    "                     'ytick.major.width':2,        # Ditto for y-ticks\n",
    "                     'xtick.direction':'in', \n",
    "                     'ytick.direction':'in'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e566de",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "A decision tree is a non-linear model that uses sequence of conditional statements about the input features to make a final prediction.\n",
    "The resulting model resembles a flowchart and is similar to how humans might make decisions, such as:\n",
    "\n",
    "![decision tree class](../../assets/fig/week_1/04/decision_tree_class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e758b31",
   "metadata": {},
   "source": [
    "_Note that the above is only an illustration and we do not advocate skipping class_. üòú\n",
    "\n",
    "In any case, you'll note that the decision tree starts at the **root node** and there is a binary **split** at each decision node (blue ellipses) in the tree. \n",
    "The **features** used to make the decisions _can repeat_ (they do not in this example), and it can be a categorical feature (some class or binary, like the `is it recorded?` node) _or_ a numerical feature (some threshold, like the `when did I sleep?` node).\n",
    "The final decision is made at the **leaf nodes**, which are colored orange and green in the above example.\n",
    "\n",
    "**Pause and reflect**: Is the above decision tree used to solve a classification or a regression problem?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2513660",
   "metadata": {},
   "source": [
    "### Benefits of decision trees\n",
    "\n",
    "There are several nice aspects to using decision trees, including:\n",
    "- Works for **classification and regression problems** (discussed more below).\n",
    "- Works with **categorical and numerical features** (discussed above).\n",
    "- **Easy to interpret** the model and how predictions are being made.\n",
    "- **Highly flexible** in how splits (decisions) are made.\n",
    "- Features **don't need to be standardized**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82009949",
   "metadata": {},
   "source": [
    "### Training a decision tree\n",
    "\n",
    "Training a decision tree is a little complicated so we won't delve into the details, but as you might imagine, the cost function for regression trees can be our familiar mean squared error.\n",
    "However, this cost function is now _local_ and re-evaluated at each decision node, so the decision tree construction is a **greedy** procedure.\n",
    "For classification tasks, the cost function changes to a different metric, common ones being the [Gini impurity](https://victorzhou.com/blog/gini-impurity/) or the [information gain (entropy)](https://victorzhou.com/blog/information-gain/).\n",
    "Notably, based on the cost function, the algorithm **learns the best feature** to split on at each node for the \"optimal split\" based on the training data; the user does not specify exactly which feature to consider.\n",
    "The user does have to supply an input design matrix and an output target vector, which makes a decision tree a **supervised learning** algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb165f",
   "metadata": {},
   "source": [
    "### Decision trees in scikit-learn\n",
    "\n",
    "OK, now that we have a little bit of the theory under our belts, let's jump into building decision trees.\n",
    "The [`sklearn.tree`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) module contains the [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) classes.\n",
    "\n",
    "You may notice that these constructors have quite a lot of input arguments, which gives us our first experience working with **hyperparameters**. \n",
    "Recall that our model _parameters_ ($\\vec{\\theta}$ in linear regression, or the splitting criteria from above) are _learned from the data_ through an optimization procedure.\n",
    "Hyperparameters, on the other hand, are _chosen by the user_ and typically _optimized using a cross-validation procedure_ where we search through a list of candidate values for each hyperparameter and pick the best one based on the validation error.\n",
    "This is called **hyperparameter optimization** and is really important to building a good ML model.\n",
    "\n",
    "One such hyperparameter we might choose is the `max_depth` which determines how many layers our tree will have (the scikit-learn default is as many as necessary).\n",
    "In the example below, we will return to the atomic weight regression problem with the atomic number as the single, numerical feature, and you can see how changing `max_depth` changes the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce85027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor       # ML model\n",
    "from ipywidgets import interact, IntSlider, Layout   # cool package for interactivity\n",
    "\n",
    "df = pd.read_csv('../../assets/data/week_1/04/elem_props.csv')\n",
    "X = df[['atomic_number']]\n",
    "y = df['atomic_mass']\n",
    "\n",
    "def train_and_plot(depth):\n",
    "    dt = DecisionTreeRegressor(max_depth=depth)\n",
    "    dt.fit(X, y)\n",
    "    y_hat = dt.predict(X)\n",
    "    train_err = mean_squared_error(y_hat, y, squared=False)\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    y_pred = cross_val_predict(dt, X, y, cv=kfold)\n",
    "    val_err = mean_squared_error(y_pred, y, squared=False)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X, y, label='data')\n",
    "    ax.plot(X, y_hat, 'k', label='predictions')\n",
    "    ax.text(1, 235, f'training RMSE: {train_err:.3f}')\n",
    "    ax.text(1, 210, f'validation RMSE: {val_err:.3f}')\n",
    "    ax.set_xlabel('atomic number')\n",
    "    ax.set_ylabel('atomic mass')\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "depth_widget = IntSlider(value=1, min=1, max=8, step=1,\n",
    "                         description='Maximum tree depth',\n",
    "                         style={'description_width':'150px'}, continuous_update=True,\n",
    "                         layout=Layout(width='400px', height='30px'))\n",
    "\n",
    "interact(train_and_plot, depth=depth_widget);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30cf09",
   "metadata": {},
   "source": [
    "**Pause and reflect**: Why is the prediction line so jagged?\n",
    "\n",
    "**Answer**: After the maximum depth has been reached, it will only predict the mean of the remaining training points in that leaf node.\n",
    "\n",
    "----\n",
    "\n",
    "Hopefully you saw that by increasing the `max_depth` of the decision tree, we can get better and better fits to our training data, eventually decreasing the _training error_ all the way down to 0!\n",
    "\n",
    "**Pause and reflect**: What drawback of decision trees does this behavior suggest?\n",
    "\n",
    "<!-- solution-begin -->\n",
    "**Answer**: High variance! Decision trees are _extremely sensitive_ to the input data.\n",
    "<!-- solution-begin -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ca048f",
   "metadata": {},
   "source": [
    "### Visualizing a decision tree\n",
    "\n",
    "Similar to our very first example where we visualized our \"decision tree\" for attending class, we can programmatically create visualizations of our decision tree models using the [`plot_tree(decision_tree)`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) function in the `sklearn.tree` module.\n",
    "This is relatively new function that is quite customizable, giving us the option to specify the feature labels and class labels.\n",
    "We demonstrate it below for the classification problem of a metallic element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "df = pd.read_csv('../../assets/data/week_1/04/elem_props.csv')\n",
    "features = ['row', 'group']\n",
    "X = df[features]\n",
    "y = df['is_metal']\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "dt.fit(X, y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_tree(dt, feature_names=features, class_names=['non-metal', 'metal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec4feb",
   "metadata": {},
   "source": [
    "Now you can see exactly how the tree made each decision for classifying whether a material was a metal or non-metal.\n",
    "For example, it makes sense that the first split is vertical between groups 13 and 14, where if the group number is 13 or less, we proceed down a \"metal\" branch.\n",
    "The leaf nodes are those without arrows underneath, and the class label that's shown is the label that will be predicted if a material lands on that leaf as a result of traversing the tree.\n",
    "Notice that features can be repeated, even in successive layers of the tree.\n",
    "**Hooray for interpretability**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45ee38",
   "metadata": {},
   "source": [
    "## Random forests\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "We hope that you've enjoyed the presentation on decision trees.\n",
    "It's a cool, intuitive, and interpretable ML algorithm that's good to know.\n",
    "And it makes for some pretty visuals. üôÇ\n",
    "\n",
    "But there is one _huge problem_ with decision trees and that is their **high variance**.\n",
    "As you saw, decision trees are capable of partitioning the training data in a very intricate way, which means that the decision tree is very sensitive to noise/error in the training data, which can often lead to **overfitting** or simply inaccuracies.\n",
    "Once again, it seems like we're at the mercy of the **bias-variance tradeoff** (ha!), where we're either going to end up with large errors due to high-bias/low-complexity models (like linear regression and logistic regression) or high-variance/high-complexity models (like decision trees, or neural networks).\n",
    "There are ways of improving the performance of decision trees using hyperparameter optimization or _pruning_ (eliminating branches of the tree), but _it would be great if there was an algorithm that could land us at the sweet spot without too much hand tuning_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4389e",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Luckily for us, there _is_ such an algorithm, which we will try to motivate in this section.\n",
    "\n",
    "**Idea 1**: Because we're proud of democracy, we might consider an algorithm that trains not one, not two, but _many_ decision trees and then have them \"vote\" on the final prediction.\n",
    "By \"vote,\" we mean we take the _mode_ of the predictions from every decision tree if it's a _classification_ task, and the _mean_ of the predictions from every decision tree if it's a _regression_ problem.\n",
    "In other words, we are **aggregating** the outputs and performance of an **ensemble** of decision trees‚Äîtwo heads are better than one.\n",
    "\n",
    "**Idea 2**: But wait!\n",
    "If we use the same training data for training each decision tree in this ensemble model, we're actually going to end up with pretty similar (**correlated**) trees... which means we'll perform the same as using just one decision tree.\n",
    "How can we inject some variation... maybe with some randomness?\n",
    "Let's **bootstrap** the training data, which means from the $m$ data points in our whole training dataset, we will **randomly sample with replacement** $m$ examples to form the training dataset for decision tree #1, randomly sample a different set of $m$ examples for training decision tree #2, etc.\n",
    "\n",
    "**Idea 3**: The fun doesn't have to stop there.\n",
    "We know that when we have a lot of features (degrees of freedom), we increase the model complexity and run the risk of overfitting to the training data.\n",
    "So, maybe when we're deciding which feature to split on at each node, we can just consider **a subset of the features** instead of all the features.\n",
    "\n",
    "This subset can be a randomly-sampled portion (w/o replacement is fine) of the features at each decision node in every decision tree.\n",
    "\n",
    "Combining ideas 1 and 2 is called **bagging** (**b**ootstrap **agg**regat**ing**), and when we combine them with idea 3 to create an ensemble model of decision trees, we get one of the üêê ML algorithms: **random forests**.\n",
    "An earlier version of this algorithm was invented by [Tin Kam Ho](https://researcher.watson.ibm.com/researcher/view.php?person=us-tho) in 1995, while more-or-less the exact version described here was [invented in 2001 by Leo Breiman](https://link.springer.com/article/10.1023/A:1010933404324), who was a UC Berkeley PhD student ('54) and Professor of Statistics for 25 years! üêª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa347c",
   "metadata": {},
   "source": [
    "### Random forests in scikit-learn\n",
    "\n",
    "Scikit-learn has a large collection of [ensemble models](https://scikit-learn.org/stable/modules/ensemble.html), among which are the [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "You'll note that these classes have many of the same input parameters as the decision tree classes, with a few additional ones due to ensembling.\n",
    "We invite you to read the documentation carefully to understand what each input means.\n",
    "The default values for the constructor aren't bad.\n",
    "\n",
    "In the cell below, we use the `RandomForestRegressor` to model the bandgap prediction problem that was presented in the \"Featurization\" notebook.\n",
    "We purposefully restrict the set of features using the following code (everything else is the same):\n",
    "```python\n",
    "features = ['Number', 'MendeleevNumber', 'AtomicWeight', 'MeltingT', \n",
    "            'Column', 'Row', 'CovalentRadius', 'Electronegativity', \n",
    "            'NsValence', 'NpValence', 'NdValence', 'NfValence', 'NValence', \n",
    "            'NsUnfilled', 'NpUnfilled', 'NdUnfilled', 'NfUnfilled', 'NUnfilled', \n",
    "            'GSvolume_pa', 'GSbandgap', 'GSmagmom', 'SpaceGroupNumber']\n",
    "stats = ['mean']   # we can also add 'minimum', 'maximum', 'range', 'avg_dev', 'mode'\n",
    "featurizer = ElementProperty(data_source='magpie',\n",
    "                             features=features,\n",
    "                             stats=stats)\n",
    "# featurizer = ElementProperty.from_preset('magpie')   # instead of this\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and construct the arrays\n",
    "df = pd.read_csv('../../assets/data/week_1/04/band_gap_featurized.csv')\n",
    "display(df.head())\n",
    "X = df.loc[:, 'MagpieData mean Number':]\n",
    "y = df['gap expt']\n",
    "\n",
    "# Train the model and perform CV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "y_hat = cross_val_predict(rf, X, y, cv=kfold)\n",
    "\n",
    "# Plot the parity plot\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.scatter(y, y_hat, s=30)\n",
    "ax.plot(y, y, 'k', zorder=-5)\n",
    "ax.set_xlabel('actual band gap (eV)')\n",
    "ax.set_ylabel('predicted band gap (eV)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12761208",
   "metadata": {},
   "source": [
    "The predictions still aren't perfect (band gap is a hard problem!), but they're better than our linear regression model.\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "One of the cool aspects of random forests is that we can calculate the **importance score of each feature** that measures a degree of its \"influence\" on predicting the output property.\n",
    "The importance scores will be normalized to sum to `1.0`, and you can read more online about exactly how this is done, if you want.\n",
    "The code below will do this for the band gap prediction problem up above and rank the features in order of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d14894",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)                                              # we have to explicitly fit the model first\n",
    "features = np.array([s[11:] for s in X.columns])          # remove the \"MagpieData \" prefix\n",
    "importances = rf.feature_importances_                     # get the importances from the attribute\n",
    "importances_sorted = sorted(importances, reverse=False)   # sort importances, ascending for plotting quirk\n",
    "indices_sorted = np.argsort(importances)                  # get the indices for indexing into features\n",
    "features_sorted = features[indices_sorted]                # sort the features too for labeling the plot\n",
    "\n",
    "# create the bar chart for importance scores in ranked order\n",
    "yvals = np.arange(len(indices_sorted))\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(yvals, importances_sorted)\n",
    "ax.set_yticks(yvals)\n",
    "ax.set_yticklabels(features_sorted)\n",
    "ax.set_ylabel('feature')\n",
    "ax.set_xlabel('importance score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc646883",
   "metadata": {},
   "source": [
    "**Pause and reflect**: Can you rationalize the feature importances above?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a04e42",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <center><b>~ BREAK ~</b></center>\n",
    "    At this point, we highly suggest you give yourself a decent break before continuing further. ü•∞\n",
    "    Get a drink of water, go grab a bite, or at least stand up and stretch to give your eyes a break.\n",
    "    You all are awesome!! üôå\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b497b2",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Up until this point, we've only looked at supervised learning algorithms, where the algorithm must be given the inputs $X$ and outputs $\\vec{y}$ to fit some parameters $\\vec{\\theta}$.\n",
    "Now we will launch into a short discussion of **unsupervised learning** algorithms, where the algorithm is only given the [same] input design matrix $X$, _but not the output target vector_ $\\vec{y}$.\n",
    "We'll discuss some concrete examples of such algorithms and why we might want to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b60a8a",
   "metadata": {},
   "source": [
    "### Goals of unsupervised learning\n",
    "\n",
    "After reading the above, one might think, \"Well, if an unsupervised learning algorithm is provided even less information than a supervised learning algorithm, wouldn't it necessarily be less powerful?\"\n",
    "This would be true if the algorithms were trying to achieve the same goal, but since they aren't, unsupervised learning algorithms can still be very useful.\n",
    "Recall in supervised learning, we're trying to figure out how to map a set of inputs in $X$ to an output $\\vec{y}$ as accurately as possible.\n",
    "In unsupervised learning, we're no longer concerned about any target property/label, but rather we're curious about the **inherent structure** in the data, such as whether there are clusters or trends along certain features.\n",
    "The goal always comes first, then the algorithms (which scientists decided to name this way)‚Äînot the other way around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d9811",
   "metadata": {},
   "source": [
    "## $k$-means clustering\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Since one of the goals of unsupervised learning is to reveal clusters in your data, let's discuss one of the simplest algorithms to achieve this goal: $k$**-means**.\n",
    "The intuition behind this algorithm is to find $k$ cluster centers, or **centroids**, such that all data points that are closest to the same centroid are assigned the same cluster.\n",
    "Note that we say two data points belong to the same cluster, we're **not** saying they have the same class/label (like in classification problems), but rather that they are near each other _in feature space_.\n",
    "\n",
    "$k$-means is an **iterative algorithm** consisting of two steps:\n",
    "\n",
    "0. First, at the very beginning, the user initializes $k$ centroids by their choosing.\n",
    "1. **Assignment step**: For each _data point_, assign it to the closest centroid by Euclidean distance.\n",
    "2. **Update step**: For each _centroid_, recalculate its position to be the centroid of all the data points that were assigned to it in the previous step.\n",
    "3. Repeat steps **1** and **2** until the centroids converge.\n",
    "\n",
    "To illustrate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2978acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402393b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60a6b6b9",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef39565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26030fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51e529c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "This concludes the bonus content on advanced ML models.\n",
    "\n",
    "If you have any questions, please do not hesitate to ask on Slack!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
