{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5334554e",
   "metadata": {},
   "source": [
    "# Quantifying performance notebook\n",
    "\n",
    "*Authors: Enze Chen and Mark Asta (University of California, Berkeley)*\n",
    "\n",
    "```{note}\n",
    "This is an interactive exercise, so you will want to click the {fa}`rocket` and open the notebook in DataHub (or Colab for non-UCB students).\n",
    "```\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "This notebook contains a series of exercises that will teach you ways we evaluate ML model performance in the train-validate-test paradigm.\n",
    "We will rely heavily on modules in the [scikit-learn package](https://scikit-learn.org/stable/) to implement these concepts.\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Use error metrics and visualizations to assess model performance and evaluate the appropriateness of these methods in regression settings.\n",
    "1. Articulate the importance of having training, validation, and test datasets.\n",
    "1. Implement these ideas, including cross-validation, using scikit-learn.\n",
    "\n",
    "We will progress through most of this exercise together as a group and are happy to take questions you might have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32d824",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "These exercises are grouped into the following sections:\n",
    "\n",
    "1. [Regression model performance](#Regression-model-performance)\n",
    "1. [Train-validate-test](#Train-validate-test)\n",
    "1. [Cross-validation](#Cross-validation)\n",
    "1. [Classification model performance](#Classification-model-performance-(on-your-own)) (bonus)\n",
    "\n",
    "### Import Python packages\n",
    "\n",
    "Please remember to run the following cell before continuing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac709c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(8,6),       # Increase figure size\n",
    "                     'font.size':20,               # Increase font size\n",
    "                     'mathtext.fontset':'cm',      # Change math font to Computer Modern\n",
    "                     'mathtext.rm':'serif',        # Documentation recommended follow-up\n",
    "                     'lines.linewidth':5,          # Thicker plot lines\n",
    "                     'lines.markersize':12,        # Larger plot points\n",
    "                     'axes.linewidth':2,           # Thicker axes lines (but not too thick)\n",
    "                     'xtick.major.size':8,         # Make the x-ticks longer (our plot is larger!)\n",
    "                     'xtick.major.width':2,        # Make the x-ticks wider\n",
    "                     'ytick.major.size':8,         # Ditto for y-ticks\n",
    "                     'ytick.major.width':2,        # Ditto for y-ticks\n",
    "                     'xtick.direction':'in', \n",
    "                     'ytick.direction':'in'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1615ce50",
   "metadata": {},
   "source": [
    "## Regression model performance\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Like the previous lesson, we'll talk about regression and classification problems separately, as they will have different performance metrics.\n",
    "Also like in the previous lesson, we'll spend quite some time sketching and discussing these concepts live using an iPad, so we'll only offer a quick summary here and use the notebook mostly for live demonstrations and hands-on practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66810e",
   "metadata": {},
   "source": [
    "### Error metrics: RMSE and MAE\n",
    "\n",
    "In the last notebook, we used the squared loss for the cost function, and we can do something similar to get the **root-mean-square error (RMSE)**.\n",
    "The mathematical expression for the RMSE between the target vector $\\vec{y}$ and the predicted values $\\hat{y}$ is:\n",
    "\n",
    "$$ \\text{RMSE}(\\vec{y}, \\hat{y}) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2} $$\n",
    "\n",
    "Another popular error metric is the **mean absolute error (MAE)**, which is given by\n",
    "\n",
    "$$ \\text{MAE}(\\vec{y}, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m} | \\hat{y}_i - y_i | $$\n",
    "\n",
    "The minimum values for both of these metrics is 0, and the closer we get to this the better our predictions.\n",
    "\n",
    "----\n",
    "\n",
    "**Pause and reflect**: What are the differences between these two metrics?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bdfdaf",
   "metadata": {},
   "source": [
    "### Scikit-learn performance metrics\n",
    "\n",
    "Scikit-learn has RMSE and MAE (and many other) regression performance metrics accessible through the [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) module.\n",
    "\n",
    "The RMSE can be computed using the [`mean_squared_error(y_true, y_pred, squared=False)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) method by passing the additional parameter `squared=False`.\n",
    "\n",
    "The MAE can be computed using the [`mean_absolute_error(y_true, y_pred)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error   # syntax for multiple imports from same module\n",
    "\n",
    "# simple arrays for demonstration; you can verify results by hand!\n",
    "a = np.array([10, 1])\n",
    "b = np.array([0, 0])\n",
    "\n",
    "print(f'The RMSE is {mean_squared_error(a, b, squared=False)}.')\n",
    "print(f'The MAE is {mean_absolute_error(a, b)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa531e3",
   "metadata": {},
   "source": [
    "### Exercise: compute both error metrics for linear regression for the atomic mass prediction problem\n",
    "\n",
    "Note that unlike the previous notebook, we're giving you a lot more **features** (and many more elements) to work with, so have fun!\n",
    "You can start with just using the atomic number to predict atomic mass, and then see what other features you want to add to your design matrix to lower the error.\n",
    "\n",
    "_Hints_:\n",
    "- When selecting columns of pandas DataFrames to construct $X$, _even for a single column_, format your selection as a list. \n",
    "By which we mean:\n",
    "\n",
    "```python\n",
    "X = df[['atomic_number', 'row']]   # this is good; a list of 2 column names\n",
    "X = df[['atomic_number']]          # this is good; a list of 1 column name\n",
    "X = df['atomic_number']            # avoid this!!! a string of 1 column name\n",
    "```\n",
    "\n",
    "- Note that before fitting the data, we also **scaled the input features** to zero mean and unit variance using the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). \n",
    "    This is a best practice for linear regression and other algorithms optimized by gradient descent.\n",
    "    _Why might we want to do this?_\n",
    "- Using the original list of 50 elements (if you truncate the following) and `atomic_number` as a single feature, your RMSE/MAE will be around 1.72/1.34. \n",
    "Using the full list of 100 elements (what's loaded in) and `atomic_number` as a single feature, your RMSE/MAE will be around 3.54/2.88."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../assets/data/week_1/04/elem_props.csv')\n",
    "display(df.head())\n",
    "X = df[['atomic_number']]\n",
    "y = df['atomic_mass']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n",
    "\n",
    "# --------------------------------------------------------------- #\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y, label='data')\n",
    "ax.plot(X, lr.coef_[0] * X_scaled + lr.intercept_, 'gray', label='regression')\n",
    "ax.set_xlabel('atomic number')\n",
    "ax.set_ylabel('atomic weight')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a17e7e",
   "metadata": {},
   "source": [
    "### Visualizations: parity plots\n",
    "\n",
    "Note that previously when we visualized our regression results, we showed output vs. inputs (a single feature), where the scattered points are $y$ (truth) vs. $x$ and the regression line was $\\hat{y}$ (prediction) vs. $x$.\n",
    "The regression line is really made of 100 predictions from our model, and these predicted points are connected by a line, forming the line of best fit.\n",
    "\n",
    "Another way to visualize the results, particularly when there are many input features, is with a **parity plot** or **predicted vs. actual plot**.\n",
    "As the second name implies, this plot shows _a scatter plot_ of the predicted target values (from the model) on the $y$-axis and the actual target values (from the original dataset) on the $x$-axis.\n",
    "A perfect model (error = 0.0) would have a parity plot where all the points lie along $y=x$ (all predicted points exactly match their true values).\n",
    "\n",
    "### Exercise: please create a parity plot for your results from the previous part\n",
    "\n",
    "_Hints_: \n",
    "- We recommend making the marker sizes smaller, by passing `s=40` into the `ax.scatter()` method.\n",
    "- One typically adds the line $y=x$ in the background of such a plot, to help guide the eye. \n",
    "To make the line appear _under the points_, pass `zorder=-5` as an input parameter to [`ax.plot()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html).\n",
    "- One also typically makes the axes of such figures equal, to emphasize that there should be a sense of \"perfect $y=x$ agreement.\"\n",
    "We can use the [`ax.set_aspect('equal')`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_aspect.html) method to do this!\n",
    "- Definitely need axes labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f95ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300403e",
   "metadata": {},
   "source": [
    "**Pause and reflect**: For what atomic weights are our linear regression model predicting _too high_? \n",
    "Where is it predicting _too low_?\n",
    "This is how the parity plot helps with **interpretability** of our model predictions!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a7dfb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <center><b>~ BREAK ~</b></center>\n",
    "    At this point, we're going to give ourselves a short break before continuing further.\n",
    "    Great work so far! üí™\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e5c42",
   "metadata": {},
   "source": [
    "## Train-validate-test\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "So far our workflow has been something like this:\n",
    "\n",
    "1. Gather some data $X$ and $\\vec{y}$ for the purposes of training the supervised ML model.\n",
    "We call these data **training data**.\n",
    "1. Fit (train) the ML model on these data, $X$ and $\\vec{y}$, to learn the correct parameters $\\vec{\\theta}$.\n",
    "1. Evaluate the ML model on $X$ to get the **training error**.\n",
    "\n",
    "While this methodology is fine, it can be **misleading** in that we'll think our ML model is doing better than it really is. \n",
    "This is because we're evaluating its performance on data that it has already seen (been trained on), instead of _new, unseen data_ that would be a better test of the model's true predictive power.\n",
    "It would also be much more in the spirit of materials discovery, since any new materials that we synthesize and want to evaluate are presumably new materials that have never been made before.\n",
    "\n",
    "This paradigm will also matter a great deal for more complex ML models, if you choose to use them, that can \"memorize\" the training data really well.\n",
    "Imagine studying for an exam with practice problems, and then seeing those exact same questions appear on the real exam!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519e89d",
   "metadata": {},
   "source": [
    "Given this, it is common to split our full dataset into **three parts**:\n",
    "\n",
    "1. The **training data**, $X_{\\text{train}}$ and $\\vec{y}_{\\text{train}}$, used for training our model's parameters.\n",
    "1. The **validation data**, $X_{\\text{val}}$ and $\\vec{y}_{\\text{val}}$, used for evaluating our trained model's performance.\n",
    "The trained model's performance on the validation data is the validation error.\n",
    "Depending on how satisfied we are with this, we might adjust our ML model, and repeat the training-validation loop.\n",
    "1. The **test data**, $X_{\\text{test}}$ and $\\vec{y}_{\\text{test}}$, used for evaluating our _final_ trained model's performance. \n",
    "Ideally (in theory), you will run your model on the test data _only once_, after all your validation iterations have finished!\n",
    "The error incurred at this step is the **test error**. \n",
    "This is the error that everyone will care about and the one you should _always report_!\n",
    "\n",
    "There's no real guidelines on how much of your data to allocate to each set.\n",
    "Some common heuristics are 60/20/20 (train/val/test), 50/20/30, and 50/25/25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ad0be",
   "metadata": {},
   "source": [
    "### Splitting data in scikit-learn\n",
    "\n",
    "How might you create these divisions?\n",
    "A very simple way would just be to take (for a 50/20/30 split) the first 50% of the rows (examples) in $X$ _and_ $\\vec{y}$ to be your training data.\n",
    "Then take the next 20% of the rows to be your validation data.\n",
    "Then use the remaining 30% to be your test data.\n",
    "This could work, but it seems a little clunky to have to manually calculate the index of the splits, and we might actually induce **bias** by using adjacent examples, depending on how our data were collected/organized.\n",
    "\n",
    "If we look inside the [`sklearn.model_selection`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) module, we see there's a handy [`train_test_split(arrays, test_size=0.25, shuffle=True)`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function that will take a sequence of arrays and produce a randomized (shuffled) train/test split of **each** array according to the proportion specified with the `test_size` parameter.\n",
    "For example, to execute a 50/20/30 split, one could do:\n",
    "```python\n",
    "X_tv, X_test, y_tv, y_test = train_test_split(X, y, test_size=0.30)             # first split off test data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tv, y_tv, test_size=0.20)   # then split train/val data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b129e",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "As we know, in MI our datasets are often quite small (sometimes only dozens of data points), so it might not seem very practical to conduct a training/validation/test split when we ideally want to maximize the amount of training data available for training our ML models.\n",
    "It is pretty much ML dogma that **the more training data we can use, the better‚Äîalways**.\n",
    "A common practice to maximally leverage our data while allowing for validation is through **cross-validation (CV)**.\n",
    "\n",
    "![cross_validation](../../assets/fig/week_1/04/cross_validation.png)\n",
    "\n",
    "In particular, we will be discussing a form of CV known as $k$**-fold CV**, where the data is randomly split into $k$ folds (groups), and we take turns building a ML model trained on $k-1$ folds and validating on the final fold. \n",
    "We repeat this for each of $k$ possible validation folds and average the results from all the validation trials to get the **CV score** (based on a chosen performance metric)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de8cd8",
   "metadata": {},
   "source": [
    "### scikit-learn for cross validation\n",
    "\n",
    "If you thought splitting a dataset once into training/validation was hard, then imagine doing it $k$ times.\n",
    "Luckily, scikit-learn has [so many options](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) to help with CV that it's actually hard to make sense of them all. üòÖ\n",
    "We'll be looking at two of them:\n",
    "\n",
    "- [`KFold(n_splits=5, shuffle=False)`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) is used to _construct an object_ that saves our CV settings.\n",
    "The arguments listed here are the default values for the constructor.\n",
    "- [`cross_val_score(estimator, X, y, scoring, cv)`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) is a _function_ that takes many arguments, and below we describe the five we think will be most relevant:\n",
    "    - `estimator`: the ML model used to fit the data.\n",
    "    - `X`: the design matrix.\n",
    "    - `y`: the target vector (for supervised learning).\n",
    "    - `scoring`: a metric for evaluating performance, where _higher values are always better_. \n",
    "    There are many options, which you can see [here](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values).\n",
    "    - `cv`: where the CV object goes.    \n",
    "\n",
    "This function computes and returns **a list of scores** of the estimator for each run of the CV.\n",
    "\n",
    "Later, on your own, you might be interested in the similar [`cross_val_predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20119e",
   "metadata": {},
   "source": [
    "### Exercise: perform 5-fold CV for the atomic mass prediction problem with a linear regression model\n",
    "\n",
    "_Hints_:\n",
    "- You should construct a `KFold()` object as the input to `cross_val_score()`.\n",
    "- For `scoring`, use the `'neg_root_mean_squared_error'`.\n",
    "It returns an array of scores, and you should print these.\n",
    "- Start with `shuffle=False` inside the `KFold()` constructor. \n",
    "Then change it to `True`. \n",
    "Do your scores change? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ead48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "df = pd.read_csv('../../assets/data/week_1/04/elem_props.csv')\n",
    "df.head()\n",
    "X = df[['atomic_number']]\n",
    "y = df['atomic_mass']\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n",
    "\n",
    "print(f'The CV errors are {scores}.')\n",
    "print(f'The average CV score is {-np.mean(scores)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0a85e",
   "metadata": {},
   "source": [
    "**Pause and reflect**: Why are the scores negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7656a5a",
   "metadata": {},
   "source": [
    "## Classification model performance (on your own)\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "For classification metrics, rather than measuring a difference between real values, we're more concerned about whether the predicted class matches the true class.\n",
    "Ironically, despite having fewer possible output values (they're now discrete class labels), the types of error metrics actually increases, so there's a lot that we could discuss.\n",
    "So we only have time for a few of the most common ones.\n",
    "\n",
    "We will again focus on binary classification to illustrate the idea, and actually start with a visualization: the **confusion matrix**.\n",
    "For a two-class classification problem, the confusion matrix is given by \n",
    "\n",
    "|                   | Positive prediction | Negative prediction |\n",
    "| :-----------      | :-------------      | :-------------      |\n",
    "| _Positive label_: | True positive (TP)  | False negative (FN) |\n",
    "| _Negative label_: | False positive (FP) | True negative (TN)  |\n",
    "\n",
    "where TP, FP, FN, TN are typically counts given in integers.\n",
    "For the confusion matrix, the **accuracy** of a classifer is generally speaking the most intuitive metric we could calculate, given by:\n",
    "\n",
    "$$ \\text{accuracy}\\ = \\frac{\\text{TP + TN}}{\\text{TP + FP + FN + TN}} \\in [0, 1] $$\n",
    "\n",
    "which is interpreted as \"the number of correct classifications out of all data points.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699ccd7",
   "metadata": {},
   "source": [
    "### Scikit-learn performance metrics\n",
    "\n",
    "Scikit-learn can compute confusion matrices and accuracy (and many other) classification performance metrics using functions in the [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) module.\n",
    "\n",
    "The confusion matrix given a target vector $\\vec{y}$ and predicted classes $\\hat{y}$ can be computed using the function [`confusion_matrix(y_true, y_pred)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), which returns a matrix of dimension $\\text{num_classes} \\times \\text{num_classes}$ with element $(i,j)$ being the number of samples with true label $i$ and predicted label $j$.\n",
    "Note that **the order of the two input vectors matters**!\n",
    "Also, if the labels are numerical (e.g., `0`, `1`), then the output confusion matrix is sorted from smallest to largest, so it might help to explicitly pass a `labels=[1, 0]` argument to fix the label order for the final confusion matrix [to match what we had above].\n",
    "\n",
    "The accuracy can be computed by hand from the above confusion matrix, or if we want to directly compute it from the true classes $\\vec{y}$ and predicted classes $\\hat{y}$, we can use the [`accuracy_score(y_true, y_pred)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function.\n",
    "\n",
    "We again start with an example by hand to get the feel for things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y    = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "yhat = [1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
    "classes = [1, 0]\n",
    "n = len(classes)\n",
    "\n",
    "cm = confusion_matrix(y, yhat, labels=classes)\n",
    "acc = accuracy_score(y, yhat)\n",
    "print(f'The confusion matrix is:\\n{cm}')\n",
    "print(f'The accuracy is: {acc}')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(cm, cmap='Blues')\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        ax.text(j, i, s=cm[i, j], c='black', bbox=dict(fc='white'))  \n",
    "        # note i,j had to be swapped above for labeling text!\n",
    "        \n",
    "ax.set_xticks(np.arange(n))\n",
    "ax.set_xticklabels(classes)\n",
    "ax.set_yticks(np.arange(n))\n",
    "ax.set_yticklabels(classes)\n",
    "ax.set_xlabel('predicted class')\n",
    "ax.set_ylabel('true class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb599d",
   "metadata": {},
   "source": [
    "### Exercise: compute both performance metrics for logistic regression for the metal classification problem\n",
    "\n",
    "_Hints_:\n",
    "- In this problem, you should use `lr.predict(X)` to get discrete predicted class labels instead of probabilities.\n",
    "- Feel free to change up the features and see if you can increase your accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classes = [True, False]\n",
    "n = len(classes)\n",
    "df = pd.read_csv('../../assets/data/week_1/04/elem_props.csv')\n",
    "display(df.head())\n",
    "# -------------   WRITE YOUR CODE IN THE SPACE BELOW   ---------- #\n",
    "\n",
    "# --------------------------------------------------------------- #\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(cm, cmap='Blues')\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        ax.text(j, i, s=cm[i, j], c='black', bbox=dict(fc='white'))  \n",
    "        # note i,j had to be swapped above for labeling text!\n",
    "        \n",
    "ax.set_xticks(np.arange(n))\n",
    "ax.set_xticklabels(classes)\n",
    "ax.set_yticks(np.arange(n))\n",
    "ax.set_yticklabels(classes)\n",
    "ax.set_xlabel('predicted class')\n",
    "ax.set_ylabel('true class')\n",
    "ax.set_title('confusion matrix for \"is metal?\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b746d2a3",
   "metadata": {},
   "source": [
    "### Caveat: F1 score üèé (on your own time)\n",
    "\n",
    "If we had a little more time, or for those who are interested, we would discuss how accuracy _is not the best performance metric for classification_.\n",
    "The biggest reason is due to **dataset imbalance**, which is most often the case with real-world data (materials and otherwise), such as the following: \n",
    "Imagine you do very well (20/20) for positive labels, and pretty well (900/980) for negative labels.\n",
    "In other words, the confusion matrix is:\n",
    "\n",
    "|                   | Positive prediction | Negative prediction |\n",
    "| :-----------      | :-------------      | :-------------      |\n",
    "| _Positive label_: | 20 TP  | 0 FN |\n",
    "| _Negative label_: | 80 FP | 900 TN  |\n",
    "\n",
    "What is your accuracy?\n",
    "92%, which is not bad.\n",
    "**But**, you might look at this classifier and think, \"Uh oh, it's not picking negative labels that well, which would be a huge problem if those false positives corresponded to a disastrous consequence, like an electric car battery exploding.\" üò¨\n",
    "\n",
    "Therefore, we need a different metric(s), and the most common metrics you'll see in the literature for classification are actually **precision**, **recall**, and **F1 score**.\n",
    "\n",
    "The _precision_ is the number of True positives out of all positive predictions, given by\n",
    "\n",
    "$$ \\frac{\\text{TP}}{\\text{TP + FP}} $$\n",
    "\n",
    "It is useful when we want to _minimize false positives_ (since we want to maximize precision).\n",
    "\n",
    "The _recall_ is the number of True positives out of all actual positives, given by\n",
    "\n",
    "$$ \\frac{\\text{TP}}{\\text{TP + FN}} $$\n",
    "\n",
    "It is useful when we want to _minimize false negatives_ (since we want to maximize recall).\n",
    "\n",
    "In most cases, there's a tradeoff between precision and recall, which is nicely illustrated in [this lesson from Google Developers](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall#precision-and-recall:-a-tug-of-war).\n",
    "To balance these two distinct performance criteria, we can compute the **F1 score**, which is the harmonic mean of the two quantities:\n",
    "\n",
    "$$ F_1 = \\frac{2}{\\text{precision}^{-1} + \\text{recall}^{-1}} $$\n",
    "\n",
    "If you want, you can experiment with the appropriate functions in [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) to calculate these quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e529c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This concludes the exercises on model performance evaluation. \n",
    "In the third leg of this introduction to ML, we'll take a look at **featurization** for MI applications, which continues to build on this knowledge.\n",
    "Therefore, please do let us know if you have questions about the material so far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
